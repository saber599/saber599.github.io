[{"title":"CPU100排查","url":"/2023/11/01/CPU100%E6%8E%92%E6%9F%A5/","content":"背景接手的代码在运行时cpu抛高\n排查过程1. 确定进程通过top命令找出cpu抛高的进程\ntop\n\n\n2. 确定线程通过top -Hp 进程号找出cpu抛高的线程\ntop -Hp 39511\n\n\n3. 确定线程正在执行内容3.1. 查看堆栈\n将线程号转为16进制\n\nprintf &quot;%x\\n&quot; 40907\n\n\n\n通过jstack查找线程堆栈信息\n\njstack 39511 | grep -A 50 9fcb\n\n\n3.2. 查看日志一般情况我们通过查看堆栈信息就能定位到代码出问题所在位置，但这里堆栈信息指向的位置为一个抽象类，该抽象类下面有很多实现类，所以不好确定是哪个类有问题，故而再通过日志再定位具体位置。\ncat s.log | grep &quot;Thread-14&quot;\n\n\n4. 分析代码通过上述的排查过程最终定位到出现问题的类为AbstractQueueListener下面的实现类MonitorBrokerLeaderListener。根据下面的源码可以看出当MonitorBrokerLeaderListener执行handle后，isWorking就会一直为false，AbstractQueueListener中就会出现在循环中一直不会执行handle了，且sleep为0，即出现死循环。\n问题修复由于写代码的人已经找不到了，只能推测MonitorBrokerLeaderListener作用，该类的作用是想在leader节点间发生漂移后重新同步节点信息以及增加zookeeper节点watcher检测变化，基于该前提下将MonitorBrokerLeaderListener调整为以下内容\n","categories":["问题排查"],"tags":["cpu","100%","实战"]},{"title":"DataXOOM","url":"/2023/12/05/DataXOOM/","content":"背景线上环境发现有部分从ftp采集数据的任务出现OOM情况\n排查过程1. 查看日志通过日志看到有出现OOM异常，因DataX的启动脚本配置了DEFAULT_JVM = “-Xms1g -Xmx1g -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=%s/log” % (DATAX_HOME)，故而出现OOM会输出对应的dump文件\n2. 使用MAT分析dump文件\n\n使用MAT打开dump文件，根据报告发现大对象：com.csvreader.CsvReader\n\n2. 查看大对象对应属性，发现该对象的rawBuffer和columnBuffer较大\n3. 查看DataX源码查看代码，发现CsvReader会默认将两个”之间的内容视为一列，在出现第一个”后即使遇到列分割符也不会结束列的读取，必须等到下一个”才结束，到这就已经知道原因了\n问题修复后续文件读取类任务，默认不开启该操作，都给csv配置带上： “useTextQualifier”: false\n思考\njava应用都应该在启动命令上加上HeapDumpOnOutOfMemoryError HeapDumpPath参数，便于后续出现OOM问题排查\n使用新组件时，需要了解关键默认配置\n\n","categories":["问题排查"],"tags":["实战","DataX","OOM"]},{"title":"DataX文本抽取支持字符串切割","url":"/2024/01/05/DataX%E6%96%87%E6%9C%AC%E6%8A%BD%E5%8F%96%E6%94%AF%E6%8C%81%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%88%87%E5%89%B2/","content":"背景DataX的文本抽取，都是基于com.csvReader.CsvReader进行读取，CsvReader有个缺陷就是无法对文本按照字符串进行切割，但是部分业务数据的行分隔符，列分隔符是使用的字符串\n方案1. 将字符串切割和单字符切割分开实现阿里云Dataphin的官方方案\n优点：\n\n方案简单\n\n缺点：\n\n对DataX源码侵入比较大，改动范围较广\n行分隔符仅支持\\r\\n和\\n，不能自定义字符串行分割符\n使用字符串的split对性能有一定影响\n\n2. 改造CsvReader，支持字符串切割优点：\n\n对DataX源码侵入较小，仅改动CsvReader即可\n可以同时支持列分隔符、行分隔符自定义字符串切割\n性能比使用字符串的split好\n\n缺点：\n\n实现难度较高，需要花时间进行边界值测试\n\n结论：考虑到后续的源码维护、升级，综合考虑，采用第二种方案\n实现\n","categories":["组件二开"],"tags":["实战","DataX"]},{"title":"hystrix配置参数","url":"/2021/08/23/Hystrix%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0/","content":"HystrixCommand\n配置方式我们的配置都是基于 HystrixCommand 的，我们通过在方法上添加@HystrixCommand 注解并配置注解的参数来实现配置，但有的时候一个类里面会有多个 Hystrix 方法，每个方法都是类似配置的话会冗余很多代码，这时候我们可以在类上使用@DefaultProperties注解来给整个类的 Hystrix 方法设置一个默认值。\n配置项下面是 HystrixCommand 支持的参数，除了 commandKey/observableExecutionMode/fallbackMethod 外，都可以使用@DefaultProperties配置默认值。\n\ncommandKey：用来标识一个 Hystrix 命令，默认会取被注解的方法名。需要注意：Hystrix 里同一个键的唯一标识并不包括 groupKey，建议取一个独一二无的名字，防止多个方法之间因为键重复而互相影响。\ngroupKey：一组 Hystrix 命令的集合， 用来统计、报告，默认取类名，可不配置。\nthreadPoolKey：用来标识一个线程池，如果没设置的话会取 groupKey，很多情况下都是同一个类内的方法在共用同一个线程池，如果两个共用同一线程池的方法上配置了同样的属性，在第一个方法被执行后线程池的属性就固定了，所以属性会以第一个被执行的方法上的配置为准。\ncommandProperties：与此命令相关的属性。\nthreadPoolProperties：与线程池相关的属性，\nobservableExecutionMode：当 Hystrix 命令被包装成 RxJava 的 Observer 异步执行时，此配置指定了 Observable 被执行的模式，默认是 ObservableExecutionMode.EAGER，Observable 会在被创建后立刻执行，而 ObservableExecutionMode.EAGER模式下，则会产生一个 Observable 被 subscribe 后执行。我们常见的命令都是同步执行的，此配置项可以不配置。\nignoreExceptions：默认 Hystrix 在执行方法时捕获到异常时执行回退，并统计失败率以修改熔断器的状态，而被忽略的异常则会直接抛到外层，不会执行回退方法，也不会影响熔断器的状态。\nraiseHystrixExceptions：当配置项包括 HystrixRuntimeException 时，所有的未被忽略的异常都会被包装成 HystrixRuntimeException，配置其他种类的异常好像并没有什么影响。\nfallbackMethod：方法执行时熔断、错误、超时时会执行的回退方法，需要保持此方法与 Hystrix 方法的签名和返回值一致。\ndefaultFallback：默认回退方法，当配置 fallbackMethod 项时此项没有意义，另外，默认回退方法不能有参数，返回值要与 Hystrix方法的返回值相同。\n\ncommandProperties\n配置方式Hystrix 的命令属性是由 @HystrixProperty 注解数组构成的，HystrixProperty 由 name 和 value 两个属性，数据类型都是字符串。\n以下将所有的命令属性分组来介绍。 \n线程隔离(Isolation)\nexecution.isolation.strategy： 配置请求隔离的方式，有 threadPool（线程池，默认）和 semaphore（信号量）两种，信号量方式高效但配置不灵活，我们一般采用 Java 里常用的线程池方式。\nexecution.timeout.enabled：是否给方法执行设置超时，默认为 true。\nexecution.isolation.thread.timeoutInMilliseconds：方法执行超时时间，默认值是 1000，即 1秒，此值根据业务场景配置。\nexecution.isolation.thread.interruptOnTimeout： execution.isolation.thread.interruptOnCancel：是否在方法执行超时/被取消时中断方法。需要注意在 JVM 中我们无法强制中断一个线程，如果 Hystrix 方法里没有处理中断信号的逻辑，那么中断会被忽略。\nexecution.isolation.semaphore.maxConcurrentRequests：默认值是 10，此配置项要在 execution.isolation.strategy 配置为 semaphore 时才会生效，它指定了一个 Hystrix 方法使用信号量隔离时的最大并发数，超过此并发数的请求会被拒绝。信号量隔离的配置就这么一个，也是前文说信号量隔离配置不灵活的原因。\n\n统计器(Metrics)滑动窗口： Hystrix 的统计器是由滑动窗口来实现的，我们可以这么来理解滑动窗口：一位乘客坐在正在行驶的列车的靠窗座位上，列车行驶的公路两侧种着一排挺拔的白杨树，随着列车的前进，路边的白杨树迅速从窗口滑过，我们用每棵树来代表一个请求，用列车的行驶代表时间的流逝，那么，列车上的这个窗口就是一个典型的滑动窗口，这个乘客能通过窗口看到的白杨树就是 Hystrix 要统计的数据。\n桶： bucket 是 Hystrix 统计滑动窗口数据时的最小单位。同样类比列车窗口，在列车速度非常快时，如果每掠过一棵树就统计一次窗口内树的数据，显然开销非常大，如果乘客将窗口分成十分，列车前进行时每掠过窗口的十分之一就统计一次数据，开销就完全可以接受了。 Hystrix 的 bucket （桶）也就是窗口 N分之一 的概念。\n\nmetrics.rollingStats.timeInMilliseconds：此配置项指定了窗口的大小，单位是 ms，默认值是 1000，即一个滑动窗口默认统计的是 1s 内的请求数据。\nmetrics.healthSnapshot.intervalInMilliseconds：它指定了健康数据统计器（影响 Hystrix 熔断）中每个桶的大小，默认是 500ms，在进行统计时，Hystrix 通过 metrics.rollingStats.timeInMilliseconds / metrics.healthSnapshot.intervalInMilliseconds 计算出桶数，在窗口滑动时，每滑过一个桶的时间间隔时就统计一次当前窗口内请求的失败率。\nmetrics.rollingStats.numBuckets：Hystrix 会将命令执行的结果类型都统计汇总到一块，给上层应用使用或生成统计图表，此配置项即指定了，生成统计数据流时滑动窗口应该拆分的桶数。此配置项最易跟上面的 metrics.healthSnapshot.intervalInMilliseconds 搞混，认为此项影响健康数据流的桶数。 此项默认是 10，并且需要保持此值能被 metrics.rollingStats.timeInMilliseconds 整除。\nmetrics.rollingPercentile.enabled：是否统计方法响应时间百分比，默认为 true 时，Hystrix 会统计方法执行的 1%,10%,50%,90%,99% 等比例请求的平均耗时用以生成统计图表。\nmetrics.rollingPercentile.timeInMilliseconds：统计响应时间百分比时的窗口大小，默认为 60000，即一分钟。\nmetrics.rollingPercentile.numBuckets：统计响应时间百分比时滑动窗口要划分的桶用，默认为6，需要保持能被metrics.rollingPercentile.timeInMilliseconds 整除。\nmetrics.rollingPercentile.bucketSize：统计响应时间百分比时，每个滑动窗口的桶内要保留的请求数，桶内的请求超出这个值后，会覆盖最前面保存的数据。默认值为 100，在统计响应百分比配置全为默认的情况下，每个桶的时间长度为 10s = 60000ms / 6，但这 10s 内只保留最近的 100 条请求的数据。\n\n熔断器(Circuit Breaker)\ncircuitBreaker.enabled：是否启用熔断器，默认为 true;\ncircuitBreaker.forceOpen： circuitBreaker.forceClosed：是否强制启用/关闭熔断器，强制启用关闭都想不到什么应用的场景，保持默认值，不配置即可。\ncircuitBreaker.requestVolumeThreshold：启用熔断器功能窗口时间内的最小请求数。试想如果没有这么一个限制，我们配置了 50% 的请求失败会打开熔断器，窗口时间内只有 3 条请求，恰巧两条都失败了，那么熔断器就被打开了，5s 内的请求都被快速失败。此配置项的值需要根据接口的 QPS 进行计算，值太小会有误打开熔断器的可能，值太大超出了时间窗口内的总请求数，则熔断永远也不会被触发。建议设置为 QPS * 窗口秒数 * 60%。\ncircuitBreaker.errorThresholdPercentage：在通过滑动窗口获取到当前时间段内 Hystrix 方法执行的失败率后，就需要根据此配置来判断是否要将熔断器打开了。 此配置项默认值是 50，即窗口时间内超过 50% 的请求失败后会打开熔断器将后续请求快速失败。\ncircuitBreaker.sleepWindowInMilliseconds：熔断器打开后，所有的请求都会快速失败，但何时服务恢复正常就是下一个要面对的问题。熔断器打开时，Hystrix 会在经过一段时间后就放行一条请求，如果这条请求执行成功了，说明此时服务很可能已经恢复了正常，那么会将熔断器关闭，如果此请求执行失败，则认为服务依然不可用，熔断器继续保持打开状态。此配置项指定了熔断器打开后经过多长时间允许一次请求尝试执行，默认值是 5000。\n\n其他(Context/Fallback)\nrequestCache.enabled：是否启用请求结果缓存。默认是 true，但它并不意味着我们的每个请求都会被缓存。缓存请求结果和从缓存中获取结果都需要我们配置 cacheKey，并且在方法上使用 @CacheResult 注解声明一个缓存上下文。\nrequestLog.enabled：是否启用请求日志，默认为 true。\nfallback.enabled：是否启用方法回退，默认为 true 即可。\nfallback.isolation.semaphore.maxConcurrentRequests：回退方法执行时的最大并发数，默认是10，如果大量请求的回退方法被执行时，超出此并发数的请求会抛出 REJECTED_SEMAPHORE_FALLBACK 异常。\n\nthreadPoolProperties\n配置方式线程池的配置也是由 HystrixProperty 数组构成，配置方式与命令属性一致。\n配置项\ncoreSize：核心线程池的大小，默认值是 10，一般根据 QPS * 99% cost + redundancy count 计算得出。\nallowMaximumSizeToDivergeFromCoreSize：是否允许线程池扩展到最大线程池数量，默认为 false;\nmaximumSize：线程池中线程的最大数量，默认值是 10，此配置项单独配置时并不会生效，需要启用 allowMaximumSizeToDivergeFromCoreSize 项。\nmaxQueueSize：作业队列的最大值，默认值为 -1，设置为此值时，队列会使用 SynchronousQueue，此时其 size 为0，Hystrix 不会向队列内存放作业。如果此值设置为一个正的 int 型，队列会使用一个固定 size 的 LinkedBlockingQueue，此时在核心线程池内的线程都在忙碌时，会将作业暂时存放在此队列内，但超出此队列的请求依然会被拒绝。\nqueueSizeRejectionThreshold：由于 maxQueueSize 值在线程池被创建后就固定了大小，如果需要动态修改队列长度的话可以设置此值，即使队列未满，队列内作业达到此值时同样会拒绝请求。此值默认是 5，所以有时候只设置了 maxQueueSize 也不会起作用。\nkeepAliveTimeMinutes：由上面的 maximumSize，我们知道，线程池内核心线程数目都在忙碌，再有新的请求到达时，线程池容量可以被扩充为到最大数量，等到线程池空闲后，多于核心数量的线程还会被回收，此值指定了线程被回收前的存活时间，默认为 2，即两分钟。\n\n工作方式Hystrix 内线程池的使用是基于 Java 内置线程池的简单包装，通常有以下三种状态：\n\n如果请求量少，达不到 coreSize，通常会使用核心线程来执行任务。\n如果设置了 maxQueueSize，当请求数超过了 coreSize, 通常会把请求放到 queue 里，待核心线程有空闲时消费。\n如果 queue 长度无法存储请求，则会创建新线程执行直到达到 maximumSize 最大线程数，多出核心线程数的线程会在空闲时回收。\n\n引用\n原文：https://www.cnblogs.com/zhenbianshu/p/9630167.html\n","categories":["hystrix"],"tags":["hystrix","配置"]},{"title":"Seatunnel测试","url":"/2023/06/25/Seatunnel%E6%B5%8B%E8%AF%95/","content":"1. 前置条件\nseatunnel\nhadoop，检查点会用到hdfs，没有也能看到效果\n本地hadoop，需要hadoop_home，没有也能看到效果\n\n2. sftp-&gt;hive2.1 场景说明将ftp文件同步到hive分区表中，根据ftp文件内容字段动态保存。\n\n2.2 准备\n环境准备\n\nseatunnel-2.3.1\nhadoop-3.3.2\nhive-3.1.2\nsftp服务器\n\n\nftp文件内容\n\n\nuser_id#user_name#create_time#province_id1#张三#2023-06-24 19:59:23#112#李四#2023-06-23 18:50:20#113#王五#2023-06-24 19:59:23#124#赵六#2023-06-24 20:03:01#11\n\n\nhive建表sql\n\ncreate table if not exists user_info(        user_id string comment &#x27;用户id&#x27;,        user_name string comment &#x27;用户名&#x27;,        create_time timestamp comment &#x27;创建时间&#x27;) comment &#x27;用户信息&#x27;partitioned by (province_id string, create_day date)row format delimitedfields terminated by &#x27;#&#x27;location &#x27;/hive/warehouse/dev.db/user_info&#x27;;\n\n2.3 seatunnel配置文件编写env &#123;  # You can set engine configuration here  execution.parallelism = 1  job.mode = &quot;BATCH&quot;  checkpoint.interval = 5000  execution.checkpoint.data-uri = &quot;hdfs://192.168.10.222:9000/checkpoint&quot;&#125;source &#123;    # https://seatunnel.apache.org/docs/connector-v2/source/FtpFile    SftpFile &#123;        path = &quot;/opt/upload/user_info.txt&quot;        host = &quot;192.168.10.222&quot;        port = 22        user = hadoop        password = hadoop        file_format_type = &quot;text&quot;        schema = &#123;            fields &#123;              user_id = string              user_name = string              create_time = TIMESTAMP              province_id = string              &#125;        &#125;        delimiter = &quot;#&quot;        datetime_format = &quot;yyyy-MM-dd HH:mm:ss&quot;        skip_header_row_number = 1        result_table_name = tmp      &#125;&#125;transform &#123;  # https://seatunnel.apache.org/docs/transform-v2/sql  # https://seatunnel.apache.org/docs/transform-v2/sql-functions    Sql &#123;        source_table_name = &quot;tmp&quot;        result_table_name = &quot;tmp2&quot;        query = &quot;select user_id, user_name, create_time, TO_CHAR(create_time, &#x27;yyyy-MM-dd&#x27;) create_day, province_id  from tmp&quot;      &#125;&#125;sink &#123;  # https://seatunnel.apache.org/docs/connector-v2/sink/HdfsFile  HdfsFile &#123;      source_table_name = &quot;tmp2&quot;      fs.defaultFS = &quot;hdfs://192.168.10.222:9000&quot;      path = &quot;/hive/warehouse/dev.db/user_info&quot;      file_format = &quot;text&quot;      field_delimiter = &quot;\\t&quot;      have_partition = true      partition_by = [&quot;province_id&quot;,&quot;create_day&quot;]      partition_dir_expression = &quot;$&#123;k0&#125;=$&#123;v0&#125;/$&#123;k1&#125;=$&#123;v1&#125;&quot;      is_partition_field_write_in_file = true      custom_filename = true      file_name_expression = &quot;$&#123;transactionId&#125;_$&#123;now&#125;&quot;      filename_time_format = &quot;yyyy.MM.dd&quot;      sink_columns = [&quot;user_id&quot;,&quot;user_name&quot;,&quot;create_time&quot;,&quot;create_day&quot;,&quot;province_id&quot;]      is_enable_transaction = true  &#125;&#125;\n\n2.4 多种方式加载数据到hive\n上传数据到指定分区目录后修复，将没有创建分区的目录创建对应分区\n\nmsck repair table user_info;\n\n\n事先/是否创建分区即可\n\nalter table user_info add partition(province_id=&#x27;11&#x27;,create_day=&#x27;2023-06-23&#x27;);alter table user_info add partition(province_id=&#x27;11&#x27;,create_day=&#x27;2023-06-24&#x27;);alter table user_info add partition(province_id=&#x27;12&#x27;,create_day=&#x27;2023-06-24&#x27;);\n\n\n手动执行load语句load数据\n\nload data inpath &#x27;hdfs://192.168.10.222:9000/hive/warehouse/dev.db/user_info/province_id=11/create_day=2023-06-23/\tT_724967131367604225_a452435c6e_0_1_2023.06.24_0.txt&#x27; into table user_info partition(province_id=&#x27;11&#x27;,create_day=&#x27;2023-06-23&#x27;);\n\n2.5 执行结果\nseatunnel执行结果https://cdn.jsdelivr.net/gh/saber599/image@main/blog_images/seatunnel测试用例/be57584da985150eec5d47fa5dfa0a82.4i8swjae99c0.webp\n\nhdfs结果\n\n\n\n\n\n\n\nhive查询结果\n\n\n\n3. kafka-&gt;hive3.1 场景说明将kafka队列数据同步到hive分区表中，根据消息体字段动态保存。\n3.2 准备\n环境准备\n\nseatunnel-2.3.1\nhadoop-3.3.2\nhive-3.1.2\nkafka_2.11-0.11.0.1\n\n\nkafka内容\n\n\n&#123;&quot;user_id&quot;:&quot;1&quot;,&quot;user_name&quot;:&quot;张三&quot;,&quot;create_time&quot;:&quot;2023-06-24 19:59:23&quot;,&quot;province_id&quot;:&quot;12&quot;&#125;&#123;&quot;user_id&quot;:&quot;2&quot;,&quot;user_name&quot;:&quot;李四&quot;,&quot;create_time&quot;:&quot;2023-06-23 18:50:20&quot;,&quot;province_id&quot;:&quot;11&quot;&#125;&#123;&quot;user_id&quot;:&quot;3&quot;,&quot;user_name&quot;:&quot;王五&quot;,&quot;create_time&quot;:&quot;2023-06-24 19:59:23&quot;,&quot;province_id&quot;:&quot;12&quot;&#125;&#123;&quot;user_id&quot;:&quot;4&quot;,&quot;user_name&quot;:&quot;赵六&quot;,&quot;create_time&quot;:&quot;2023-06-24 20:03:01&quot;,&quot;province_id&quot;:&quot;11&quot;&#125;\n\n\nhive建表sql\n\ncreate table if not exists user_info_kafka(        user_id string comment &#x27;用户id&#x27;,        user_name string comment &#x27;用户名&#x27;,        create_time timestamp comment &#x27;创建时间&#x27;) comment &#x27;用户信息&#x27;partitioned by (province_id string, create_day date)row format delimitedfields terminated by &#x27;#&#x27;location &#x27;/hive/warehouse/dev.db/user_info_kafka&#x27;;\n\n3.3 seatunnel配置文件编写env &#123;  # You can set engine configuration here  execution.parallelism &#x3D; 1  job.mode &#x3D; &quot;STREAMING&quot;  # job.mode &#x3D; &quot;BATCH&quot;  checkpoint.interval &#x3D; 5000  execution.checkpoint.data-uri &#x3D; &quot;hdfs:&#x2F;&#x2F;192.168.10.222:9000&#x2F;checkpoint&quot;&#125;source &#123;    # https:&#x2F;&#x2F;seatunnel.apache.org&#x2F;docs&#x2F;connector-v2&#x2F;source&#x2F;kafka    Kafka &#123;        result_table_name &#x3D; &quot;tmp&quot;        schema &#x3D; &#123;          fields &#123;              user_id &#x3D; string              user_name &#x3D; string              create_time &#x3D; string              province_id &#x3D; string          &#125;        &#125;        # consumer.group &#x3D; &quot;test_group&quot;        # format &#x3D; text        datetime_format &#x3D; &quot;yyyy-MM-dd HH:mm:ss&quot;        format &#x3D; json        # field_delimiter &#x3D; &quot;#&quot;        topic &#x3D; &quot;user_info_topic&quot;        bootstrap.servers &#x3D; &quot;192.168.10.222:9092&quot;#        kafka.config &#x3D; &#123;#              client.id &#x3D; client_1#              max.poll.records &#x3D; 500#              auto.offset.reset &#x3D; &quot;earliest&quot;#              enable.auto.commit &#x3D; &quot;false&quot;#            &#125;      &#125;&#125;transform &#123;  # https:&#x2F;&#x2F;seatunnel.apache.org&#x2F;docs&#x2F;transform-v2&#x2F;sql  # https:&#x2F;&#x2F;seatunnel.apache.org&#x2F;docs&#x2F;transform-v2&#x2F;sql-functions    Sql &#123;        source_table_name &#x3D; &quot;tmp&quot;        result_table_name &#x3D; &quot;tmp2&quot;        query &#x3D; &quot;select user_id, user_name, create_time, SUBSTR(create_time, 0,10) create_day, province_id  from tmp&quot;      &#125;&#125;sink &#123;  # https:&#x2F;&#x2F;seatunnel.apache.org&#x2F;docs&#x2F;connector-v2&#x2F;sink&#x2F;HdfsFile  HdfsFile &#123;      source_table_name &#x3D; &quot;tmp2&quot;      fs.defaultFS &#x3D; &quot;hdfs:&#x2F;&#x2F;192.168.10.222:9000&quot;      path &#x3D; &quot;&#x2F;hive&#x2F;warehouse&#x2F;dev.db&#x2F;user_info_kafka&quot;      file_format &#x3D; &quot;text&quot;      field_delimiter &#x3D; &quot;\\t&quot;      have_partition &#x3D; true      partition_by &#x3D; [&quot;province_id&quot;,&quot;create_day&quot;]      partition_dir_expression &#x3D; &quot;$&#123;k0&#125;&#x3D;$&#123;v0&#125;&#x2F;$&#123;k1&#125;&#x3D;$&#123;v1&#125;&quot;      is_partition_field_write_in_file &#x3D; true      custom_filename &#x3D; true      file_name_expression &#x3D; &quot;$&#123;transactionId&#125;_$&#123;now&#125;&quot;      filename_time_format &#x3D; &quot;yyyy.MM.dd&quot;      sink_columns &#x3D; [&quot;user_id&quot;,&quot;user_name&quot;,&quot;create_time&quot;,&quot;create_day&quot;,&quot;province_id&quot;]      is_enable_transaction &#x3D; true  &#125;&#125;\n\n3.4 发送kafka消息\n启动zk\n\nnohup bin/zookeeper-server-start.sh config/zookeeper.properties &gt;&gt; zookeeper.nohup.out &amp;\n\n\n启动kafka\n\nnohup bin/kafka-server-start.sh config/server.properties &gt;&gt; producer.nohup.out &amp;\n\n\n创建topic\n\nbin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic user_info_topic\n\n\n发送消息\n\nbin&#x2F;kafka-console-producer.sh --broker-list localhost:9092 --topic user_info_topic\n\n\n3.5 多种方式加载数据到hive同2.4\n3.6 执行结果\nseatunnel执行结果\n\n\n\nhdfs结果\n\n\n\n\n\nhive查询结果\n\n\n4. rdb-&gt;hive4.1 场景说明将关系型数据库数据同步到hive分区表中，根据消息体字段动态保存，示例选用mysql-&gt;hive。\n4.2 准备\n环境准备\nseatunnel-2.3.1\nhadoop-3.3.2\nhive-3.1.2\nmysql-8.0.32\n\n\nmysql建表sql\n\nCREATE TABLE `user_info`  (  `user_id` bigint NOT NULL AUTO_INCREMENT COMMENT &#x27;用户id&#x27;,  `user_name` varchar(255) NOT NULL COMMENT &#x27;用户名&#x27;,  `create_time` datetime NOT NULL COMMENT &#x27;创建时间&#x27;,  `province_id` bigint NOT NULL COMMENT &#x27;省份id&#x27;,  PRIMARY KEY (`user_id`)) ENGINE = InnoDB CHARACTER SET = utf8mb4 COMMENT = &#x27;用户信息&#x27;;\n\n\nmysql表内容INSERT INTO `user_info` (`user_id`, `user_name`, `create_time`, `province_id`) VALUES (1, &#x27;张三&#x27;, &#x27;2023-06-24 19:59:23&#x27;, 12);INSERT INTO `user_info` (`user_id`, `user_name`, `create_time`, `province_id`) VALUES (2, &#x27;李四&#x27;, &#x27;2023-06-23 18:50:20&#x27;, 11);INSERT INTO `user_info` (`user_id`, `user_name`, `create_time`, `province_id`) VALUES (3, &#x27;王五&#x27;, &#x27;2023-06-24 19:59:23&#x27;, 12);INSERT INTO `user_info` (`user_id`, `user_name`, `create_time`, `province_id`) VALUES (4, &#x27;赵六&#x27;, &#x27;2023-06-24 20:03:01&#x27;, 11);\nhive建表sql\n\ncreate table if not exists user_info_mysql(        user_id string comment &#x27;用户id&#x27;,        user_name string comment &#x27;用户名&#x27;,        create_time timestamp comment &#x27;创建时间&#x27;) comment &#x27;用户信息&#x27;partitioned by (province_id string, create_day date)row format delimitedfields terminated by &#x27;#&#x27;location &#x27;/hive/warehouse/dev.db/user_info_mysql&#x27;;\n\n4.3 seatunnel配置文件编写env &#123;  # You can set engine configuration here  execution.parallelism = 1  job.mode = &quot;BATCH&quot;  checkpoint.interval = 5000  execution.checkpoint.data-uri = &quot;hdfs://192.168.10.222:9000/checkpoint&quot;&#125;source &#123;    # https://seatunnel.apache.org/docs/connector-v2/source/Mysql    Jdbc &#123;        url = &quot;jdbc:mysql://192.168.10.222:3306/seatunnel_test&quot;        driver = &quot;com.mysql.cj.jdbc.Driver&quot;        connection_check_timeout_sec = 100        user = &quot;root&quot;        password = &quot;root&quot;        query = &quot;select user_id,user_name,create_time,date_format(create_time, &#x27;%Y-%m-%d&#x27;) create_day,province_id from user_info&quot;        &#125;&#125;sink &#123;  # https://seatunnel.apache.org/docs/connector-v2/sink/HdfsFile  HdfsFile &#123;      fs.defaultFS = &quot;hdfs://192.168.10.222:9000&quot;      path = &quot;/hive/warehouse/dev.db/user_info_mysql&quot;      file_format = &quot;text&quot;      field_delimiter = &quot;\\t&quot;      have_partition = true      partition_by = [&quot;province_id&quot;,&quot;create_day&quot;]      partition_dir_expression = &quot;$&#123;k0&#125;=$&#123;v0&#125;/$&#123;k1&#125;=$&#123;v1&#125;&quot;      is_partition_field_write_in_file = true      custom_filename = true      file_name_expression = &quot;$&#123;transactionId&#125;_$&#123;now&#125;&quot;      filename_time_format = &quot;yyyy.MM.dd&quot;      sink_columns = [&quot;user_id&quot;,&quot;user_name&quot;,&quot;create_time&quot;,&quot;create_day&quot;,&quot;province_id&quot;]      is_enable_transaction = true  &#125;&#125;\n\n4.4 多种方式加载数据到hive同2.4\n4.5 执行结果\nseatunnel执行结果\n\n\n\nhdfs结果\n\n\n\n\n\nhive查询结果\n\n\n5. 结论\n支持sftp、kafka、rdb数据源\n支持根据字段动态分区\nkafka支持离线、实时处理\nsftp不支持增量扫描\nsftp不支持编码转换\nsftp不支持文件级check、解码、回传\n\n","categories":["Seatunnel"],"tags":["sftp","mysql","kafka","hive"]},{"title":"jvm参数调优实战","url":"/2021/04/20/jvm%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%E5%AE%9E%E6%88%98/","content":"背景线上有个用于对接第三方电商平台接口的duubo服务，公司内部系统需要调用第三方平台接口时不用去关心平台接口，只需要调用该服务接口即可。该服务上线后一直没有关注其gc情况，最近得空，看下是否有优化空间。\n现状在启动参数上加上打印gc日志参数，使用gceasy进行gc日志分析。\n-XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:./gc.log\n堆内存gc曲线图如下：\n图1-1\n\n\n\n图1-2\n\n通过上面两张图可以看出该服务进行FullGc次数较多，并且在进行FullGc前，老年代急剧上升，这肯定是需要进行优化的。\n该服务有四个节点，起始jvm参数如下：\n-Xms4096m -Xmx4096m -Xmn1024m -XX:-UseAdaptiveSizePolicy -XX:SurvivorRatio=8 -XX:-OmitStackTraceInFastThrow\n\n为什么老年代急剧上升晋升老年代方式的方式有：\n\n大对象直接进入老年代\n长期存活的对象进入老年代\n动态对象年龄判定\n空间分配担保\n\n由于jdk8默认收集器Parallel Scavenge不会出现大对象直接分配到老年代情况故而从下面两种情况分析；在默认参数下，长期存活的对象需要进入老年代需要经历15次MinorGc，在没有长时间驻留线程下，不会出现经历15次MinorGc后大量年轻代对象进入到老年代；故而初步判定老年代急剧上升的原因可能有两种：\n\n在大量调用下，又由于Survivor区较小，在动态对象年龄判定时，很容易满足相同年龄对象内存和大于Survivor区内存一半，提前晋升老年代\n在大量调用下，又由于Survivor区较小，Eden区早早的被充满进行MinorGc，但由于存活对象Survivor区放不下，提前晋升老年代\n\n调整根据初步分析原因可两方面进行参数调整，增加新生代内存 or 增加Survivor区内存由于该服务有四个节点，在负载均衡的前提下，可大致认为四个节点环境相同，对四个节点的参数进行不同的调整，进行比较。\n节点A：原始配置\n-Xms4096m -Xmx4096m -Xmn1024m -XX:-UseAdaptiveSizePolicy -XX:SurvivorRatio=8 -XX:-OmitStackTraceInFastThrow -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:./gc.log\n节点B：调整新生代内存为1.5G\n-Xms4096m -Xmx4096m -Xmn1536m -XX:-UseAdaptiveSizePolicy -XX:SurvivorRatio=8 -XX:-OmitStackTraceInFastThrow -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:./gc.log\n节点C：调整SurvivorRatio为6\n-Xms4096m -Xmx4096m -Xmn1024m -XX:-UseAdaptiveSizePolicy -XX:SurvivorRatio=6 -XX:-OmitStackTraceInFastThrow -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:./gc.log\n节点D：调整新生代内存为1.5G &amp; 调整SurvivorRatio为6\n-Xms4096m -Xmx4096m -Xmn1536m -XX:-UseAdaptiveSizePolicy -XX:SurvivorRatio=6 -XX:-OmitStackTraceInFastThrow -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:./gc.log\n结果分析四个节点运行25h后GC相关图表\nGC Pause Statistics\n\n\n\n\n\\\nYoung\nSurvivor\n总次数\n总时长\n平均时长\n最小时长\n最大时长\n\n\n\n节点A\n1 gb\n102.4 mb\n1885\n3 min 26 sec 831 ms\n110 ms\n20.0 ms\n2 sec 130 ms\n\n\n节点B\n1.5 gb\n153.6 mb\n1335\n2 min 37 sec 850 ms\n118 ms\n20.0 ms\n1 sec 420 ms\n\n\n节点C\n1 gb\n204.8 mb\n2029\n3 min 6 sec 250 ms\n91.8 ms\n10.0 ms\n2 sec 200 ms\n\n\n节点D\n1.5 gb\n307.2 mb\n1437\n2 min 37 sec 850 ms\n110 ms\n20.0 ms\n290 ms\n\n\nMinor GC stats\n\n\n\n\n\\\nYoung\nSurvivor\n总次数\n总时长\n平均时长\n最小时长\n最大时长\n\n\n\n节点A\n1 gb\n102.4 mb\n1870\n3 min 8 sec 870 ms\n101 ms\n20.0 ms\n230 ms\n\n\n节点B\n1.5 gb\n153.6 mb\n1330\n2 min 34 sec 680 ms\n116 ms\n20.0 ms\n250 ms\n\n\n节点C\n1 gb\n204.8 mb\n2023\n3 min 100 ms\n89.0 ms\n10.0 ms\n220 ms\n\n\n节点D\n1.5 gb\n307.2 mb\n1434\n2 min 37 sec 380 ms\n110 ms\n20.0 ms\n290 ms\n\n\nFull GC stats\n\n\n\n\n\\\nYoung\nSurvivor\n总次数\n总时长\n平均时长\n最小时长\n最大时长\n\n\n\n节点A\n1 gb\n102.4 mb\n15\n17 sec 960 ms\n1 sec 197 ms\n100 ms\n2 sec 130 ms\n\n\n节点B\n1.5 gb\n153.6 mb\n5\n3 sec 170 ms\n634 ms\n70.0 ms\n1 sec 420 ms\n\n\n节点C\n1 gb\n204.8 mb\n6\n6 sec 150 ms\n1 sec 25 ms\n70.0 ms\n2 sec 200 ms\n\n\n节点D\n1.5 gb\n307.2 mb\n3\n470 ms\n157 ms\n90.0 ms\n260 ms\n\n\n堆内存GC前数据：\n\n图A1\n\n\n图B1\n\n\n图C1\n\n\n图D1\n\n老年代晋升数据：\n\n图A2\n\n\n图B2\n\n\n图C2\n\n\n图D2\n\n从图B2及图C2中可以看到在正常情况下，晋升老年代的数量是非常小的，在7日下午13:43-14:05期间，老年代迅速增长（图B1,图C1）中也可以看出，其原因确实是这段时间产生了大量调用，通过线上日志也可以看出\n从上述图表可以看出：\n\n新生代调整为1.5G或调整SurvivorRatio为6都能满足服务日常运行，一旦出现大量调用则还是会发生老年代迅速增长情况，但是比原始配置好得多。\n从图D2中可以看出，在Survivor区大小为307.2 Mb左右能避免老年代迅速增长\n单独看MinorGc，新生代调整1.5g后虽然平均gc时长增加了一点，但是gc次数及gc总时长减少了不少\n单独看MinorGc，新生代同为1.5G下，SurvivorRatio为6时,虽然gc次数及gc总时长多了那么一点点，但是除了启动应用是进行了3次FullGc，后面都没有出现过FullGc，即使大量调用是晋升老年代也很平滑\n\n再次调整从上述结论看出，新生代增加到1.5G对有不少的提升。\n在新生代都加到1.5G前提下，进一步减少动态对象年龄判定晋升老年代数量，将Survivor区使用率调整至307.2 Mb/2=153.6Mb左右(TargetSurvivorRatio默认为50，可用jinfo -flag 查看)\n节点A：原始配置\n-Xms4096m -Xmx4096m -Xmn1024m -XX:-UseAdaptiveSizePolicy -XX:SurvivorRatio=8 -XX:-OmitStackTraceInFastThrow -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:./gc.log\n节点B：调整TargetSurvivorRatio为80\n-Xms4096m -Xmx4096m -Xmn1536m -XX:-UseAdaptiveSizePolicy -XX:SurvivorRatio=8 -XX:TargetSurvivorRatio=80 -XX:-OmitStackTraceInFastThrow -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:./gc.log\n节点C：调整为自适应调节\n-Xms4096m -Xmx4096m -Xmn1536m -XX:+UseAdaptiveSizePolicy  -XX:-OmitStackTraceInFastThrow -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:./gc.log\n节点D：调整新生代内存为2G &amp; TargetSurvivorRatio为80\n-Xms4096m -Xmx4096m -Xmn2048m -XX:-UseAdaptiveSizePolicy -XX:SurvivorRatio=8  -XX:TargetSurvivorRatio=80 -XX:-OmitStackTraceInFastThrow -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:./gc.log","categories":["Jvm"],"tags":["实战","gc","gceasy"]},{"title":"main方法结束程序不退出排查","url":"/2023/11/02/main%E6%96%B9%E6%B3%95%E7%BB%93%E6%9D%9F%E7%A8%8B%E5%BA%8F%E4%B8%8D%E9%80%80%E5%87%BA%E6%8E%92%E6%9F%A5/","content":"背景线上环境接手的代码有使用shell命令通过java -jar直接拉起一个java进程执行main方法的场景，系统长时间运行后发现有拉起的java进程main执行完毕后没有销毁\n排查过程1. 查看日志通过日志看到main方法中有java.lang.NullPointerException异常出现，但是正常出现异常后main线程结束后，进程会被销毁，现在整个进程没被销毁，猜测还有除了main线程的其他非守护线程存活\n2. 使用arthas查看未销毁进程中线程存活情况从图中能看出，确实还有两个非守护线程存活，DestroyJavaVM线程是等待其他线程执行完毕后销毁进程的线程，该线程状态是正常的，但是图中还有一个poo1-2-thread-1线程还存活，从名称上无法确定该线程正在执行的业务，但是已经能证实之前的猜测了，就是因为还有除main线程的其他非守护线程存活导致进程未销毁，接下来就是明确线程是用于哪个业务\n3. 查看业务代码，有没有使用线程池经排查后没发现有业务在使用线程池\n4. 将业务代码抽象出伪代码，本地debug查看原因\n伪代码代码实现的业务功能为：初始化Spring IOC容器获取Spring上下文，使用启动一个bean执行业务代码，最后再退出\n\npublic class Test &#123;    public static void main(String[] args) &#123;        ExecutorBoot boot = null;        try &#123;            ApplicationContext context = new FileSystemXmlApplicationContext(new String[]&#123;&quot;classpath:conf/spring/*.xml&quot;, &quot;conf/spring/*.xml&quot;, &quot;conf/*.xml&quot;&#125;);            boot = context.getBean(ExecutorBoot.class);            boot.execute(datasource);            ExeMode datasource = DpExecutorBoot.getMode(command);            datasource.loadDataInfo(command);            LogWriter.stop(boot.job.getMeta().getId());            LogWriter.stopInitLogger();            if (!boot.execute()) &#123;                System.exit(-1);            &#125; else &#123;                System.exit(0);            &#125;        &#125; catch (Throwable ex) &#123;            LogWriter.stop(boot.job.getMeta().getId());            LogWriter.stopInitLogger();            System.exit(-1);        &#125;    &#125;&#125;\n\n\n使用idea本地debug查看堆栈信息\n\n从图中可以发现poo1-2-thread-1线程是ScheduledThreadPoolExecutor线程池中的线程，在获取task时，队列中没有task就会一直等待，从这还是无法确定具体是从哪个业务创建的线程池3. 确定ScheduledThreadPoolExecutor创建位置\n在ScheduledThreadPoolExecutor打个断点，重启再看下从哪创建的\n   \n从图中可以看出ScheduledThreadPoolExecutor是由Spring创建的，有兴趣的同学可以去看Spring的源码，打上@EnableScheduling注解后会导入SchedulingConfiguration从而注入图中堆栈信息的ScheduledAnnotationBeanPostProcessor。在Spring刷新上下完成后就会发出ContextRefreshedEvent事件，ScheduledAnnotationBeanPostProcessor会接受该事件进行task生成以及ScheduledThreadPoolExecutor生成等工作\n问题修复调整catch里面的内容，保证无NullPointerException产生，并且保证无论是否出现异常都调用System.exit()即可\n思考\n在使用java -jar执行main方法时，需要保证main方法结束后将所有的线程关闭或者直接调用System.exit()\n在使用Spring框架、线程池的情况下，最好自己指定线程池中的线程名称，后续好排查问题\n\n","categories":["问题排查"],"tags":["实战","main","假死"]},{"title":"markdown语法记录","url":"/2021/08/23/md%E8%AF%AD%E6%B3%95%E8%AE%B0%E5%BD%95/","content":"字体\n颜色&lt;font style=&quot;background: #f9f2f4&quot; color=#c7254e&gt;背景色/字体颜色&lt;/font&gt;\n示例：背景色/字体颜色\n","categories":["markdown"],"tags":["markdown","语法"]},{"title":"线程假死排查","url":"/2023/11/01/%E7%BA%BF%E7%A8%8B%E5%81%87%E6%AD%BB%E6%8E%92%E6%9F%A5/","content":"背景测试环境发现有部分业务执行一次后就不再执行了，出现了线程假死情况\n排查过程1. 查看日志通过日志看到在备份文件时有异常发生，但是代码时将该异常捕获了，正常情况是不会导致线程假死的情况，需要进一步排查\n2. 查看业务代码，先判断是否有明显bug经排查后没发现明显bug\n3. 将业务代码抽象出伪代码，本地debug查看原因\n伪代码代码实现的业务功能为：将指定文件使用多线程挪到备份目录\n\n@RunWith(JUnit4.class)@Slf4jpublic class BatchExecuteTest &#123;    private final static ExecutorService FILE_PROCESS_WORKER = Executors.newFixedThreadPool(Runtime.getRuntime().availableProcessors());    @Test    public void threadStopTest() &#123;        try &#123;            ArrayList&lt;String&gt; fileNames = Lists.newArrayList(&quot;test1.txt&quot;, &quot;test2.txt&quot;, &quot;test3.txt&quot;);            while (true) &#123;                String dsType = &quot;sftp&quot;;                String ip = &quot;192.168.130.21&quot;;                String port = &quot;22&quot;;                String username = &quot;test&quot;;                String password = &quot;test&quot;;                String connectPattern = &quot;PORT&quot;;                FileHelper ftpHelper = FileManager.getFtpHelper(&quot;&quot;, dsType, ip, port, username, password, connectPattern);                batchExecute(fileNames, fileName -&gt; &#123;                    try &#123;                        String sourcePath = Paths.get(&quot;/data1/upload/test&quot;, fileName).toString();                        String targetPath = Paths.get(&quot;/data1/upload/test/bak&quot;, fileName).toString();                        ftpHelper.move(sourcePath, targetPath);                    &#125; catch (Exception e) &#123;                        log.error(&quot;移动文件到失败&quot;, e);                    &#125;                &#125;);                ftpHelper.close();            &#125;        &#125; catch (Exception e) &#123;            log.error(e.getMessage(), e);        &#125;    &#125;    private &lt;T&gt; void batchExecute(List&lt;T&gt; processList, Consumer&lt;T&gt; consumer) throws InterruptedException &#123;        if (CollectionUtils.isEmpty(processList)) &#123;            return;        &#125;        if (processList.size() == 1) &#123;            consumer.accept(processList.get(0));            return;        &#125;        CountDownLatch countDownLatch = new CountDownLatch(processList.size());        for (T t : processList) &#123;            FILE_PROCESS_WORKER.execute(() -&gt; &#123;                try &#123;                    consumer.accept(t);                &#125; finally &#123;                    countDownLatch.countDown();                &#125;            &#125;);        &#125;        countDownLatch.await();    &#125;&#125;\n\n\n使用idea本地debug查看堆栈信息\n\n从图中能看出线程main一直卡在countDownLatch.await()，出现该情况可能会有两个原因\n\n未正常计数，有task执行完毕后，没有进行countDown，导致没有一直减到0\n有task一直未执行完，导致没有扣减数量\n\n上面两个原因，从代码层面已经能排除第一个，因为在task执行体里使用了tryfinally，只要task执行完，一定会contDown，所以极大可能是第二个原因，故而再看线程池中线程的堆栈\n\n从图中能看出线程pool-1-thread-6一直卡在wait，从这堆栈信息就已经能看出问题所在了，原因在于FileHelper所封装的ChannelSftp不是一个线程安全的类，在多线程使用同一个对象的时候会出现，有的线程无法唤醒的情况。\n问题修复该问题可以考虑两种方式去修复\n\n不使用多线程去move文件，就单线程循环move\n修改FileHelper，将ChannelSftp放入到TreadLocal中，保证每个线程是一个单独的ChannelSftp，避免出现多线程竞争\n\n综合业务场景，最终采用方案1，因为业务上就没有达到需要使用多线程去move的量，只是当时另一位同事刚好将move的逻辑放到了多线程处理中，所以采用方案一调整即可\n思考\n在使用CountDownLatch时，需要注意task因异常出现不结束的情况，会导致main线程假死\n在新增业务代码时，需关注上下午，如果方法体存在并发，则需要根据实际情况考虑并发场景\n\n","categories":["问题排查"],"tags":["实战","假死","并发编程"]}]